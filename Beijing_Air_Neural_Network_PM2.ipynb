{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#DSCI 619: Deep Learning\n",
        "## Project 2\n",
        "Symphony Hopkins"
      ],
      "metadata": {
        "id": "-Yjc-BeYEHyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "y8-BGIg0gCze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are a data scientist in ClimateChange company. We were given the Beijing Multi-Site Air Quality Datasets.\n",
        "\n",
        "Each dataset contains the following features:\n",
        "* No: row number\n",
        "* date: date of the observation in the format of year-month-day\n",
        "* hour: hour of data in this row\n",
        "* PM10: PM10 concentration (ug/m^3)\n",
        "* SO2: SO2 concentration (ug/m^3)\n",
        "* NO2: NO2 concentration (ug/m^3)\n",
        "* CO: CO concentration (ug/m^3)\n",
        "* O3: O3 concentration (ug/m^3)\n",
        "* TEMP: temperature (degree Celsius)\n",
        "* PRES: pressure (hPa)\n",
        "* DEWP: dew point temperature (degree Celsius)\n",
        "* RAIN: precipitation (mm)\n",
        "* wd: wind direction\n",
        "* WSPM: wind speed (m/s)\n",
        "\n",
        "Each dataset only contains one target:\n",
        "* PM2.5: PM2.5 concentration (ug/m^3)\n",
        "\n",
        "\n",
        "Our objective is to create an optimal deep learning model with specific hyperparameters that can help us forecast the PM2.5.\n",
        "\n",
        "Data Source: [Beijing Multi-Site Air-Quality Data Data Set](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data)\n",
        "\n"
      ],
      "metadata": {
        "id": "woDGUG4zEjM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "1YTljFGUDbwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Load the dataset, PRSA_Data.csv, into memory.**"
      ],
      "metadata": {
        "id": "78_0favUDrtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will import the necessary libraries to load the dataset."
      ],
      "metadata": {
        "id": "WTp_YeXqF4Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#connecting to google drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoyKj81PF-uF",
        "outputId": "9f851c5a-9ef3-4006-ef2f-5f332ee5a75c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob"
      ],
      "metadata": {
        "id": "Lhr8Ut4DL2vS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 12 sites in Beijing, each with their own dataset in the PRSA_Data folder. Because of this, we need to load each dataset and then combine them into a single dataframe."
      ],
      "metadata": {
        "id": "mLSZqGp4HOcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use glob to retrieve all 12 csv files in PRSA_Data Folder\n",
        "path = os.getcwd()\n",
        "csv_files = glob.glob(os.path.join('gdrive/My Drive/Colab Notebooks/Topic 2/PRSA_Data','*.csv'))\n",
        "  \n",
        "  \n",
        "# reading the csv files into dataframes and concatenating them into a single dataframe\n",
        "df = pd.concat([pd.read_csv(f) for f in csv_files ], ignore_index=True)\n",
        "\n",
        "#displaying df\n",
        "display(df.head())\n",
        "\n",
        "#checking dataframe information\n",
        "print(f'''\n",
        "Dataframe Shape: {df.shape}\n",
        "Number of Sites: {len(df['station'].unique())}\n",
        "Site Names: {df['station'].unique()}''')"
      ],
      "metadata": {
        "id": "WSmuB7OzH54T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Clean and check missing values for this dataset.**"
      ],
      "metadata": {
        "id": "9qBYb42eDwVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have created a dataframe, let's clean and check it for missing values."
      ],
      "metadata": {
        "id": "eM7Gw-MqTtuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for missing values\n",
        "df.isnull().sum(axis = 0)"
      ],
      "metadata": {
        "id": "C_I0iB1MUwSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our target, PM2.5, contains null values. Since we are creating a model to forecast PM2.5 concentration levels, we will not impute the missing values. Instead, we will delete them."
      ],
      "metadata": {
        "id": "yTPF01CXLTXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#deleting rows with missing PM2.5 values\n",
        "df.dropna(axis=0, subset = ['PM2.5'], inplace = True)"
      ],
      "metadata": {
        "id": "XAqaxdkB1k-N"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if it worked."
      ],
      "metadata": {
        "id": "-yHuXrtA19Rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for missing values\n",
        "df.isnull().sum(axis = 0)"
      ],
      "metadata": {
        "id": "6S7GvWoB2AEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will use Multiple Imputation by Chained Reaction (MICE) to deal with the missing values in the other columns. Before we do this, let's make all of the variables numerical."
      ],
      "metadata": {
        "id": "DHQiQXEE23Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking data types\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "hVyqT11n06J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, *wd* and *station* are not numerical values, so we must convert them."
      ],
      "metadata": {
        "id": "SDzox852SsN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#converting categorical features to numerical values\n",
        "cat_features = ['wd', 'station']\n",
        "factors = pd.get_dummies(df[cat_features],drop_first=True)\n",
        "factors.head()"
      ],
      "metadata": {
        "id": "Gzc1cBLKUJGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will concatenate the newly converted numerical feautures and dummy variables to the dataframe, and drop the original variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "vzrOp0SSVD7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenating converted values to dataframe\n",
        "df = df.drop(cat_features,axis=1)\n",
        "df = pd.concat([df,factors],axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "q7yhFz8YVmFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's impute the missing values."
      ],
      "metadata": {
        "id": "rjON_OGu5QMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer"
      ],
      "metadata": {
        "id": "JLvaJsB35uP-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing imputer\n",
        "mice_imputer = IterativeImputer()\n",
        "\n",
        "#imputing values\n",
        "df.iloc[:, :] = mice_imputer.fit_transform(df)\n"
      ],
      "metadata": {
        "id": "Lr6KyP4_6aZv"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the dataframe for missing values to see if the values were imputed."
      ],
      "metadata": {
        "id": "LFot6v237GsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for missing values\n",
        "df.isnull().sum(axis = 0)"
      ],
      "metadata": {
        "id": "nVYdeoFN7Mhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Split the data into 80% of training and 20% of the test dataset.**\n"
      ],
      "metadata": {
        "id": "_K6EQAIQUENu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will now split the data into training and test datasets with 80% going into the training dataset and 20% going into the test dataset. \n",
        "\n"
      ],
      "metadata": {
        "id": "4oua37izWTyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#assigning variables\n",
        "X = df.drop('PM2.5',axis=1)\n",
        "y = df['PM2.5']\n",
        "\n",
        "#splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 2021)"
      ],
      "metadata": {
        "id": "HpNyGimMWyDd"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.Preprocess the data using the normalization method to convert all features into the range of [0,1]**\n"
      ],
      "metadata": {
        "id": "CSa6D-tWD-Yz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will normalize the data to convert all of the featues into the range of [0,1]."
      ],
      "metadata": {
        "id": "iyzMraXSXrIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing library\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#creating a scaler so we can transform the data to fit within the range of [0,1]\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "#normalizing the data\n",
        "X_train= scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Ztkkwr0dXryx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Neural Network"
      ],
      "metadata": {
        "id": "x6QuWN-xEJwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.Build a neuron network with two hidden layers of 20 and 10 neurons to forecast PM2.5 using all other features and TensorFlow. Does it overfit or underfit the data? Please justify your answer.**"
      ],
      "metadata": {
        "id": "ccdtjxiAEAa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After cleaning and preparing the data, we can finally build the neural network with the following layers:\n",
        "* Input Layer: 50\n",
        "* First Hidden Layer: 20\n",
        "* Second Hidden Layer: 10\n",
        "* Output Layer: 1\n"
      ],
      "metadata": {
        "id": "ZQRif-Q4X_x7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#creating empty model\n",
        "model = keras.Sequential()\n",
        "\n",
        "#input layer has 50 neurons\n",
        "model.add(layers.Dense(50, activation='relu'))\n",
        "\n",
        "#first hidden layer has 20 neurons\n",
        "model.add(layers.Dense(20, activation='relu'))\n",
        "\n",
        "#second hidden layer has 10 neurons\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "\n",
        "#output layer has 1 neuron\n",
        "model.add(layers.Dense(1, activation='relu'))\n",
        "\n",
        "#configuring the model\n",
        "model.compile(optimizer='adam',loss='mse')"
      ],
      "metadata": {
        "id": "On7PKoYCYWur"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have setup the model, we can fit the data. While we fit the data, we will keep the history of the training and validation loss to later determine if the model is under-fitting or over-fitting the data."
      ],
      "metadata": {
        "id": "qpX3iHfOatcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#fixing the seed \n",
        "tf.random.set_seed(1)\n",
        "\n",
        "#fitting the model and saving the the history of the training and validation losses\n",
        "history = model.fit(x=X_train,y=y_train,batch_size=64,epochs=100,\n",
        "          validation_data=(X_test,y_test), verbose=0)"
      ],
      "metadata": {
        "id": "YMOgYHoQbY10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's look at the model's Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "7ogGnPxxa_oR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing library\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error"
      ],
      "metadata": {
        "id": "tpCzuLPrmOMz"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting the target values for the test dataset\n",
        "y_pred = model.predict(X_test)\n",
        "print(f'Mean Squared Error: {mean_squared_error(y_test,y_pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEHJKXYAbKlg",
        "outputId": "1a140cb3-ef88-4305-bdd5-ec46a85507e3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2576/2576 [==============================] - 4s 2ms/step\n",
            "Mean Squared Error: 327.6766379404551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, that we know the MSE, let's visualize the history of the training and validation losses by creating a lineplot."
      ],
      "metadata": {
        "id": "9JpqiDx_cCqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the history into a dataframe\n",
        "trainhist = pd.DataFrame(history.history)\n",
        "#adding epoch column to dataframe\n",
        "trainhist['epoch'] = history.epoch"
      ],
      "metadata": {
        "id": "H3yREg1vcavF"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing library\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "fMC4FE77l_7q"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting training loss vs epoch\n",
        "sns.lineplot(x='epoch', y ='loss', data =trainhist)\n",
        "#plotting validation loss vs epoch\n",
        "sns.lineplot(x='epoch', y ='val_loss', data =trainhist)\n",
        "#adding legends\n",
        "plt.legend(labels=['train_loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_RLFOt_Gc_wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can tell there is some over-fitting within this model because the validation loss is slightly greater than the training loss as the number of epochs increases towards the end. Let's see if we can improve the model by tuning the hyperparameters."
      ],
      "metadata": {
        "id": "5-KWrb90dYLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "uXuwVZ5CES8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.Tune the model using the following hyperparameters using keras-tuner:**\n",
        "* **First hidden layer with units between 20 and 50 with a step size of 5**\n",
        "* **Second hidden layer with units between 5 and 10 with a step size of 1**\n",
        "* **The dropout rate for both hidden layer is between 0.2 and 0.8 with a step size of 0.1**\n",
        "\n"
      ],
      "metadata": {
        "id": "jYKLnNe1EdIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to create an optimal model. In order to achieve this, we will tune our model using Keras Tuner to find the best hyperparameters. "
      ],
      "metadata": {
        "id": "ZZ8achzqdz7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installing keras tuner\n",
        "!pip install -q -U keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXpyteG5kNti",
        "outputId": "f937813f-5c1c-4aaf-b981-0a3818f075bd"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.6/169.6 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing library\n",
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "xqkrF7QXeH9Y"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# we need to import Dropout layers from TF\n",
        "from tensorflow.keras.layers import Dropout\n",
        "# we may use the constraints for the weights for the optimization algorithms\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "\n",
        "\n",
        "#building our model inside of a function \n",
        "#the various hyperparameters will be specificed with hp\n",
        "def model_builder(hp):\n",
        "  #creating empty model\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  #like the model we first created, the input layer will contain 50 neurons\n",
        "  model.add(layers.Dense(50, activation='relu'))\n",
        "\n",
        "  #specifying the the minimum value, maximum value, and stepsize for the first hidden layer\n",
        "  hp_units1 = hp.Int('units_1', min_value = 20, max_value = 50, step = 5)\n",
        "  #adding first hidden layer\n",
        "  model.add(layers.Dense(units = hp_units1, activation = 'relu'))\n",
        "  #specifying the drop out rate for the first hidden layer\n",
        "  hp_dropout = hp.Float('dropout_rate', min_value = 0.2, max_value = 0.8, step = 0.1)\n",
        "  #adding dropout layer for the first hidden layer\n",
        "  model.add(Dropout(rate = hp_dropout))\n",
        "\n",
        "  #specifying the the minimum value, maximum value, and stepsize for the second hidden layer\n",
        "  hp_units2 = hp.Int('units_2', min_value = 5, max_value = 10, step = 1)\n",
        "  #adding second hidden layer\n",
        "  model.add(layers.Dense(units = hp_units2, activation = 'relu'))\n",
        "  #since the dropout rate for the second hidden layer is the same as the first hidden layer we will use the same variable\n",
        "  #adding dropout layer for the second hidden layer\n",
        "  model.add(Dropout(rate = hp_dropout))\n",
        "\n",
        "  #like the model we first created, the output layer will contain 1 neuron\n",
        "  model.add(layers.Dense(1))\n",
        "\n",
        "  #configuring the model to specify the optimizer, loss function, and metrics\n",
        "  model.compile(optimizer = keras.optimizers.Adam(), loss = 'mse', metrics = [tf.keras.metrics.MeanSquaredError()]) \n",
        "  \n",
        "  #returning the model\n",
        "  return model"
      ],
      "metadata": {
        "id": "TCUGPj2DeaN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can tune the model, we need to instantiate the model first. For this case, we will use Hyperband to instantiate the model. We also want to save the search results, so we will create a new file to store in our directory."
      ],
      "metadata": {
        "id": "2VhEufaJVaxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiating the model with Hyperband\n",
        "tuner = kt.Hyperband(model_builder, \n",
        "                     objective = 'val_loss',\n",
        "                     seed = 1,\n",
        "                     max_epochs = 100, \n",
        "                     directory = 'gdrive/My Drive/Colab Notebooks/Topic 2', \n",
        "                     project_name = 'Project2_HopkinsSymphony_TuningRegression')"
      ],
      "metadata": {
        "id": "EB381ClpWEMS"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the tuner has many outputs, we will define a callback to clean the outputs at the end of every training step."
      ],
      "metadata": {
        "id": "DTjrfTCeXKsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing library\n",
        "import IPython\n",
        "# defining a new parent class for the callback\n",
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
        "  #clearing the output at the end of each training step\n",
        "  def on_train_end(*args, **kwargs):\n",
        "    IPython.display.clear_output(wait = True)"
      ],
      "metadata": {
        "id": "GOZCVyTXXlgZ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will perform a search on the defined hyperparameter space."
      ],
      "metadata": {
        "id": "vd77K2ofX9ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#performing search\n",
        "tuner.search(X_train,\n",
        "             y_train,\n",
        "             epochs = 100,\n",
        "             validation_data = (X_test,y_test),\n",
        "             callbacks = [ClearTrainingOutput()])"
      ],
      "metadata": {
        "id": "gbO9e2ZXYKpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's see what the optimal hyperparameters for the model are."
      ],
      "metadata": {
        "id": "9a8eifOaYQzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#searching for optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "#printing outputs\n",
        "print(f\"\"\"\n",
        "Optimal Number of Neurons for First Hidden Layer: {best_hps.get('units_1')}. \n",
        "Optimal Number of Neurons for Second Hidden Layer: {best_hps.get('units_2')}. \n",
        "Optimal Dropout Rate for First and Second Hidden Layer: {best_hps.get('dropout_rate')}\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "kJc_he-UYvtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we know what the optimal hyperparametres are, we will retrain the model using them."
      ],
      "metadata": {
        "id": "ZNSXP6xZaFqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# building the model with the optimal hyperparameters\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "#fitting the data\n",
        "history = model.fit(x=X_train,y=y_train,batch_size=64,epochs=100,\n",
        "          validation_data=(X_test,y_test), verbose=0)"
      ],
      "metadata": {
        "id": "C05I9R7paSzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's graph the history of the model."
      ],
      "metadata": {
        "id": "eimIxUqbgoo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the history into a dataframe\n",
        "trainhist = pd.DataFrame(history.history)\n",
        "#adding epoch column to dataframe\n",
        "trainhist['epoch'] = history.epoch\n",
        "\n",
        "#plotting training loss vs epoch\n",
        "sns.lineplot(x='epoch', y ='loss', data =trainhist)\n",
        "#plotting validation loss vs epoch\n",
        "sns.lineplot(x='epoch', y ='val_loss', data =trainhist)\n",
        "#adding legends\n",
        "plt.legend(labels=['train_loss', 'val_loss'])"
      ],
      "metadata": {
        "id": "-ym8Efi2g3Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the model by finding the MSE."
      ],
      "metadata": {
        "id": "TN_DH4ecjql3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting the target values for the test dataset\n",
        "y_pred = model.predict(X_test)\n",
        "print(f'Mean Squared Error: {mean_squared_error(y_test,y_pred)}')"
      ],
      "metadata": {
        "id": "0ujxzT_da2Je"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}